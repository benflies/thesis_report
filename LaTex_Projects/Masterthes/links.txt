http://jco.ascopubs.org/content/27/35/5924.short
http://cancerres.aacrjournals.org/content/69/5/1851.short
http://www.nature.com/modpathol/journal/v21/n2s/full/3801018a.html
http://ajcp.oxfordjournals.org/content/141/6/856.abstract
http://www.nature.com/nature/journal/v486/n7404/abs/nature11156.html
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0069604
http://www.nature.com/nrclinonc/journal/v6/n9/abs/nrclinonc.2009.111.html
http://cancerres.aacrjournals.org/content/66/8/3992.short
http://onlinelibrary.wiley.com/doi/10.1002/gcc.22047/full
http://onlinelibrary.wiley.com/doi/10.1002/gcc.20854/full
http://www.pnas.org/content/109/31/E2127.short
http://cancerdiscovery.aacrjournals.org/content/2/1/82.short
http://www.sciencedirect.com/science/article/pii/S0092867412010616
http://clincancerres.aacrjournals.org/content/16/3/790.short
http://www.sciencedirect.com/science/article/pii/S1470204510702096
http://jco.ascopubs.org/content/27/35/5931.short
http://www.sciencedirect.com/science/article/pii/S1525157812003170
http://www.nejm.org/doi/full/10.1056/nejm200412303512724
http://jco.ascopubs.org/content/26/35/5705.short
http://clincancerres.aacrjournals.org/content/11/18/6650.short
http://www.archivesofpathology.org/doi/abs/10.1043/1543-2165-133.10.1600
http://jcp.bmj.com/content/66/2/79.short
http://www.sciencedirect.com/science/article/pii/S1470204510701303
http://www.nature.com/nature/journal/v501/n7467/abs/nature12627.html
http://stm.sciencemag.org/content/scitransmed/3/111/111ra121.short
http://www.nature.com/modpathol/journal/v27/n2/abs/modpathol2013122a.html
http://www.clinchem.org/content/61/3/544.full
good for intro
http://www.sciencedirect.com/science/article/pii/S0022202X15363831
good for intro
http://download.springer.com/static/pdf/320/art%253A10.1186%252Fgb-2009-10-3-r32.pdf?originUrl=http%3A%2F%2Fgenomebiology.biomedcentral.com%2Farticle%2F10.1186%2Fgb-2009-10-3-r32&token2=exp=1459882262~acl=%2Fstatic%2Fpdf%2F320%2Fart%25253A10.1186%25252Fgb-2009-10-3-r32.pdf*~hmac=6128802f371152717cdf61e1206bc94a0ab4c62c7bfdf13e8737665d649c0aea
maybe something in results
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0004576
http://www.nature.com/bjc/journal/v101/n4/abs/6605177a.html
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.358.2436&rep=rep1&type=pdf
http://clincancerres.aacrjournals.org/content/19/7/1902.short
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0080478

Lung cancer is the leading cause of cancer-related death worldwide [1], [2], [3]. It is classified as small cell or non-small cell lung cancer (NSCLC), the latter comprising three of the most common subtypes: adenocarcinoma, squamous cell carcinoma, and neuroendocrine tumors [4]. The majority of NSCLC are diagnosed at an advanced stage with inoperable disease [5]. Therefore, in more than 85% NSCLC minimally invasive procedures must be employed to obtain diagnostic material, which is consequently represented by either small biopsies or cytology samples [5], [6]. This significantly affects the morphological and molecular characterization required for targeted therapies, whose efficacy is limited to patients with specific genetic alterations [5].

For lung adenocarcinomas, epidermal growth factor receptor (EGFR) tyrosine kinase inhibitors have been approved for treatment of tumors carrying EGFR gene mutations, and crizotinib for tumors with anaplastic lymphoma kinase (ALK) gene rearrangements [7], [8], [9], [10]. Clinical trials are ongoing in subgroups of patients harboring specific molecular alterations such as BRAF, PIK3CA or KRAS activating mutations [11], [12], [13]. Therefore, the number of predictive biomarkers to be assessed for novel targeted drugs entering into clinical practice is expected to rapidly increase [2], [10], [14].

Sanger sequencing is currently the most widely applied technique in the characterization of EGFR gene status in clinical practice [15]. Real-time PCR-based methods have been shown to efficiently detect EGFR mutations in samples containing 1% mutated cancer cells [16]. However, there is no sufficient information on the predictive ability of these techniques, since no clear correlation has been established up to now between the quantity of mutant alleles in the cancer and the extent and duration of response to therapy [16], [17]. More importantly, most methods have been developed and validated to assess single gene alterations. Massive parallel sequencing, also known as next generation sequencing (NGS) or deep sequencing, has been recently introduced and is the most sensitive approach to index multiple genes starting from a limited amount of DNA [18].


http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0064271

Multiple genetic events accumulate during the progression of colorectal carcinogenesis [1]. There are a number of molecular subtypes of colorectal cancer including microsatellite instability and chromosomal instability [2], [3]. However, the subtypes have limited predictive or prognostic value and do not influence treatment decision in the metastatic setting. In contrast, KRAS mutation is the single most important and widely used molecular test in the metastatic colorectal cancer. Mutational status of KRAS guides treatment decision because presence of the mutation can predict lack of benefit from EGFR-targeted antibodies [4].


http://cancerdiscovery.aacrjournals.org/content/1/1/44.short
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029500

has interesting things for results

http://bmcmedgenomics.biomedcentral.com/articles/10.1186/1755-8794-4-68

has interesting things for results

According to the world health organization (WHO) malignant neoplasms are the most common cause of death worldwide in 2010 [1]. We now know that human solid tumors, which account for the majority of all human cancers, result from the accumulation of numerous genetic and epigenetic alterations that finally lead to the deregulation of protein-encoding genes [2–10].

However, even though the power of next generation sequencing (NGS) technologies is enormous, remarkably few studies on cancer genomes have been published so far. This is mainly due to the fact that NGS is still relatively cost - and time - intensive and that bioinformatics analyses of tumor tissues are not only challenging, but also need a lot of time - this is likely to be the major bottleneck in the future. One solution to these drawbacks is to increase the sequencing output by focusing on coding DNA regions [11, 19, 20]. Several targeted DNA enrichment technologies to reduce sequence complexity are available [21–27]. These technologies have been mainly developed using large amounts of input DNA generated from blood samples. To identify somatic mutations in solid tumors, DNA has to be extracted from tissues; with often limited access and amounts of extracted DNA. Formalin fixed and paraffin embedded (FFPE) tissue samples, which are archived on a routine basis in pathology departments, could render more and rare conditions accessible. Although FFPE tissue was successfully used for low-coverage whole genome sequencing and copy number detection it is not known if it can be taken for SNV and InDel detection after targeted enrichment strategies [28].

https://bfg.oxfordjournals.org/content/10/6/374.full

Targeted enrichment can be useful in a number of situations where particular portions of a whole genome need to be analyzed [7]. Efficient sequencing of the complete ‘exome’ (all transcribed sequences) represents a major current application, but researchers are also focusing their experiments on far smaller sets of genes or genomic regions potentially being implicated in complex diseases [e.g. derived from genome-wide association studies (GWAS)], pharmacogenetics, pathway analysis and so on [1, 8, 9].

Targeted enrichment techniques can be characterized via a range of technical considerations related to their performance and ease of use, but the practical importance of any one parameter may vary depending on the methodological approach applied and the scientific question being asked. Arguably, the most important features of a method, which in turn reflect the biggest challenges in targeted enrichment, include: enrichment factor, ratio of sequence reads on/off target region (specificity), coverage (read depth), evenness of coverage across the target region, method reproducibility, required amount of input DNA and overall cost per target base of useful sequence data.

(i)‘Hybrid capture’: wherein nucleic acid strands derived from the input sample are hybridized specifically to preprepared DNA fragments complementary to the targeted regions of interest, either in solution or on a solid support, so that one can physically capture and isolate the sequences of interest;
(ii)‘Selective circularization’: also called molecular inversion probes (MIPs), gap-fill padlock probes and selector probes, wherein single-stranded DNA circles that include target region sequences are formed (by gap-filling and ligation chemistries) in a highly specific manner, creating structures with common DNA elements that are then used for selective amplification of the targeted regions of interest;
(iii)PCR amplification: wherein polymerase chain reaction (PCR) is directed toward the targeted regions of interest by conducting multiple long-range PCRs in parallel, a limited number of standard multiplex PCRs or highly multiplexed PCR methods that amplify very large numbers of short fragments.
The design of a targeted enrichment experiment begins with a general consideration of the target region of interest. In particular, a major obstacle for targeted enrichment is posed by repeating elements, including interspersed and tandem repeats as well as elements such as pseudogenes located within and outside the region of interest. Exclusion of repeat masked elements [11] from the targeted region is a straightforward and efficient way to reduce the recovery of undesirable products due to repeats. Furthermore, at extreme values (<25% or >65%), the guanine-cytosine (GC) content of the target region has a considerable impact on the evenness and efficiency of the enrichment [12]. This can adversely affect the enrichment of the 5′-UTR/promoter region and the first exon of genes, which are often GC rich [13]. Therefore, expectations regarding the outcome of the experiment require careful evaluation in terms of the precise target region in conjunction with the appropriate enrichment method.

The performance of a targeted enrichment experiment will also depend upon the mode and quality of processing of the input DNA sample. Having sufficient high-quality DNA is key for any further downstream handling.

All three major targeted enrichment techniques (hybrid capture, circularization and PCR) differ in terms of sample library preparation workflow enabling sequencing on any of the current NGS instruments (e.g. Illumina, Roche 454 and SOLiD). Enrichment by hybrid selection relies on short fragment library preparations (typically range from 100 to 250 bp) which are generated before hybridization to the synthetic library comprising the target region. In contrast, enrichment by PCR is performed directly on genomic DNA and thereafter are the library primers for sequencing added. Enrichment by circularization offers the easiest library preparation for NGS because the sequencing primers can be added to the circularization probe, thus eliminating the need for any further library preparation steps. Sequencing can be performed either as single read or paired-end reads of the fragment library. In general, mate–pair libraries are not used for hybridization-based targeted enrichments due to the extra complications this implies in terms of target region design.

In general, a single NGS run produces enough reads to sequence several samples enriched by one of the mentioned methods. Therefore, pooling strategies and indexing approaches are a practical way to reduce the per sample cost. Depending on the method used for targeted enrichment, different multiplexing strategies can be envisaged that enable multiplexing in different stages of the enrichment process: before, during and after the enrichment. For targeted enrichment by hybrid capture, indexing of the sample is usually performed after the enrichment but to reduce the number of enrichment reactions, the sample libraries can alternatively be indexed during the library preparations and then pooled for enrichment [15]. Enrichment by PCR and circularization offers indexing during the enrichment by using bar-coded primers in the product amplification steps [16].

The task of designing the target region is relatively straightforward, and this can be managed with web-based tools offered by UCSC, Ensembl/BioMart, etc. and spreadsheet calculations (e.g. Excel) on a personal computer. Web-based tools like MOPeD offer a more user-friendly approach for oligoncleotide probe design [17]. Far more difficult, however, is the final sequence output analysis, which needs dedicated computer hardware and software. Fortunately, great progress has recently been made in read mapping and parameter selection for this process, leading to more consistent and higher quality final results [18]. Reads generated by hybrid selection will always tend to extend into sequences beyond the target region and the longer the fragment library is, the more of these ‘near target’ sequences will be recovered. Therefore, read mapping must start with a basic decision regarding the precise definition of the on/off target boundaries, as this parameter is used for counting on/off target reads and so influences the number of sequence reads considered as on target. This problem is not so critical for enrichments based on PCR and circularization as these methods do not suffer from ‘near target’ products. Another major consideration in data analysis is the coverage needed to reliably identify sequence variants, e.g. single nucleotide polymorphisms (SNP). This depends on multiple factors such as the nature of the region of interest in question, the method used for targeted enrichment. In different reports, it has ranged from 8x coverage [19], which was the minimum coverage for reliable SNP calling and up to 200x coverage [20], in this case the total average coverage for the targeted region.

To allow meaningful comparison of enrichment methods and experiments that employ them, and to rationally decide which technologies are most suitable when designing a research project, it is important that an objective set of descriptive metrics are defined and then widely used when reporting enrichment datasets. A series of metrics need to be considered, and the importance of each can be weighted according to specific needs and objectives of any experiment. A proposal for such a set of metrics is soon to be published, and it contains the following (Nilsson et al., manuscript in preparation):
Region of interest (size): ROI;
Average read depth (in ROI): D;
Fraction of ROI sufficiently covered (at a specified D): F;
Specificity (fraction of reads in ROI): S;
Enrichment Factor (D for ROI versus D for rest of genome): EF;
Evenness (lack of bias): E and
Weight (input DNA requirement): W.

Another dimension to the problem of reliably discovering sequence variation, and one where there is perhaps a little more clarity, is the impact of different software and algorithm choices used for primary sequence data analysis (e.g. the choice of suitable genome alignment tool, filter parameters for the analysis, coverage thresholds at intended bases). It has been shown that the detection of variants depends strongly on the particular software tools employed [36]. Indeed, because current alignment and analytical tools perform so heterogeneously, the 1000 Genomes Project Consortium [37] decided to avoid calling novel SNPs unless they were discovered by at least two independent analytical pipelines. In general, unified analysis workflows can and must be developed [38] to enable the combination and processing of data produced from different machines/approaches, to at least minimize instrument-specific biases and errors that otherwise detract from making high-confidence variant base calls.

Whatever mapping and analysis approach is applied, sufficient coverage on a single base resolution ranging from 20 to 50x is usually deemed necessary for reliable detection of sequence variation [39–42]. In one simulation study, the SNP discovery performances of two NGS platforms in a specific disease gene were shown to fall rapidly when the coverage depth was below 40x [43]. In addition, all called variants should ideally be supported by data from both read orientations (forward and reverse). Some researchers further insist on obtaining at least three reads from both the forward and the reverse DNA strands (double-stranded coverage) for any nonreference base before it is called [20]. Such stringent quality control practices are surely needed to minimize error rates and the impact of random sampling variance, so that true variations and sequencing artifacts can be resolved and homozygous and heterozygous genotypes at sites of variation reliably scored.

Deep coverage alone seems not, by itself, to always be sufficient for accurate variation discovery. For example, a naïve Bayesian model for SNP calling—even with deep coverage—can lead to considerable false positive rates [38]. Thus, other stringent filtering parameters should also be applied, such as filtering out SNP calls that occur at positions with too great a coverage [44], e.g. on positions where massive pile-ups are found which are either sequencing or mapping artifacts. Increasing the number of sequenced samples (individually or multiplexed) may also result in more power to confidently call variations [45]. For instance, applying an index-based multiplexed targeted sequencing approach would remove run-to-run biases and in turn facilitate calculating error estimates for genetic polymorphism detection [46]. Computing inter-sample concordance rates at each base provides yet another way to highlight sequencing errors. Sometimes, manual read inspection is necessary to refine SNP calls, but this is time consuming unless it can be partially automated. Other useful strategies include applying index-based sample multiplexing, processing controls of known sequence (e.g. HapMap DNAs) and testing parent–child trios. These ‘multisample’ approaches allow one to estimate genotype concordance rates, detect Mendelian errors and measure allelic bias at heterozygous sites. This latter problem (systematic distortions in the recovery of one nucleotide allele over another) could be due to a bias in the targeted enrichment process, in the preparation and amplification of the sequencing library, or during sequencing or postsequencing analysis [47].

The main reason that targeted enrichment has been developed as an adjunct for NGS in recent years is that it was needed to make extensive sequencing affordable for subregions of complex genomes. The alternative of fully sequencing many complete genomes to high average coverage (~30x or higher) to enable things like genetic variation analysis, was simply not affordable. Another reason for assaying, e.g. exome rather than whole-genome sequencing is the simpler data interpretation of the former. This is a crucial consideration as it is generally much more challenging to find the functional impact of variants in noncoding regions.

Current targeted enrichment methods are not yet optimal, and must be improved if they are to be relevant for a long time to come. One fundamental problem is the lack of evenness of coverage [48], which is especially troublesome if the results are intended for diagnostic purposes. Poor evenness across regions with differing percentages of GC bases is a general problem for NGS itself [2], which directly translates into lower coverage of promoter regions and the first exon of genes as these are often GC rich. Such problems are exacerbated by GC content and other biases suffered by enrichment technologies. Therefore, for reliable results, a high coverage is invaluable—but current methods for targeting several mega base pairs might only return 60–80% of the ROI at a read depth of over 40x, and 80–90% at around 20x coverage.

The comparison of different genome partitioning methods in Figure 4 gives a real-world indication of how very divergent the results of the available methods can be. Even for the same genetic locus, processed by the same people in the same laboratory, the different enrichment methods produce very different average coverage, evenness and specificity. All four hybrid capture methods, including three solution phase methods (home made, Flexelect, SureSelect) and one solid phase method (NimbleGen) show considerable fluctuation in coverage over the targeted region of interest. Depending on the length of fragment library, off-target sequences protrude more or less into genomic regions adjacent to the target region. In comparison, the SelectorProbe enrichment shows a more even coverage for the targeted region and fluctuations in coverage are due to the number of hybridization probes designed. The PCR-based enrichment (RainDance) results in the most even coverage across the targeted region, but this is flanked by the typical high coverage reads for the primer pair used for enrichment.

Still, clinical diagnostic applications of sequencing where specific clinical questions need to be answered might favor analysis of only the relevant loci at high coverage. This has a number of advantages. First, a highly accurate answer is provided, which is required when clinicians take decisions about supplying or withholding expensive targeted biological drugs to, for instance, cancer patients. Second, a targeted sequencing approach has the advantage of focusing directly to the region of interest and therefore omitting not directly relevant genomic information. Third, an important point to consider is regulatory approval of further sequencing-based diagnostic tests. Given that regulatory approval is supplied for a dedicated and specific test that addresses a specific question, a targeted sequencing approach might be more acceptable to regulatory agencies. Hence, ultimately the adoption of enrichment methods in the sequencing field may evolve differently in the research and diagnostics fields. Indeed, the future use of sequencing for diagnostics may naturally move toward a ‘single cartridge per patient’ approach, as is the current practice for other types of molecular diagnostics.
